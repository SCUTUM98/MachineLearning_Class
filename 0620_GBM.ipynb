{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n",
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "import time\n",
    "import pandas as pd\n",
    "import 사용자행동인식데이터세트실습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_path = 'UHD/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(dir_path+'features.txt',sep='\\s+', header=None,names=['column_index','column_name'])\n",
    "feature_dup_df = pd.DataFrame(data=df.groupby('column_name').cumcount(), columns=['dup_cnt'])\n",
    "feature_dup_df = feature_dup_df.reset_index()\n",
    "ndf = pd.merge(df.reset_index(), feature_dup_df, how='outer')\n",
    "ndf['column_name'] = ndf[['column_name', 'dup_cnt']].apply(lambda x : x[0]+'_'+str(x[1]) \n",
    "                                                                if x[1] >0 else x[0] ,  axis=1)\n",
    "ndf = ndf.drop(['index'], axis=1)\n",
    "feature_name = ndf.iloc[:, 1].values.tolist()    \n",
    "x_train = pd.read_csv(dir_path+'train/X_train.txt',sep='\\s+', names=feature_name )\n",
    "x_test = pd.read_csv(dir_path+'test/X_test.txt',sep='\\s+', names=feature_name)    \n",
    "y_train = pd.read_csv(dir_path+'train/y_train.txt',sep='\\s+',header=None,names=['action'])\n",
    "y_test = pd.read_csv(dir_path+'test/y_test.txt',sep='\\s+',header=None,names=['action'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = 사용자행동인식데이터세트실습.get_human_dataset()\n",
    "\n",
    "start_time = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/test/lib/python3.11/site-packages/sklearn/ensemble/_gb.py:437: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GBM 정확도: 0.9393\n",
      "GBM 수행시간: 600.9 초\n"
     ]
    }
   ],
   "source": [
    "gb_clf = GradientBoostingClassifier(random_state=0)\n",
    "gb_clf.fit(x_train, y_train)\n",
    "gb_pred = gb_clf.predict(x_test)\n",
    "gb_accuracy = accuracy_score(y_test, gb_pred)\n",
    "\n",
    "print('GBM 정확도: {0:.4f}'.format(gb_accuracy))\n",
    "print('GBM 수행시간: {0:.1f} 초'.format(time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc:  0.9392602646759416\n"
     ]
    }
   ],
   "source": [
    "print('acc: ', accuracy_score(gb_pred, y_test))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XGBOOST\n",
    "### NOT WORKING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_breast_cancer()\n",
    "features = dataset.data\n",
    "labels = dataset.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean radius</th>\n",
       "      <th>mean texture</th>\n",
       "      <th>mean perimeter</th>\n",
       "      <th>mean area</th>\n",
       "      <th>mean smoothness</th>\n",
       "      <th>mean compactness</th>\n",
       "      <th>mean concavity</th>\n",
       "      <th>mean concave points</th>\n",
       "      <th>mean symmetry</th>\n",
       "      <th>mean fractal dimension</th>\n",
       "      <th>...</th>\n",
       "      <th>worst texture</th>\n",
       "      <th>worst perimeter</th>\n",
       "      <th>worst area</th>\n",
       "      <th>worst smoothness</th>\n",
       "      <th>worst compactness</th>\n",
       "      <th>worst concavity</th>\n",
       "      <th>worst concave points</th>\n",
       "      <th>worst symmetry</th>\n",
       "      <th>worst fractal dimension</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>17.99</td>\n",
       "      <td>10.38</td>\n",
       "      <td>122.80</td>\n",
       "      <td>1001.0</td>\n",
       "      <td>0.11840</td>\n",
       "      <td>0.27760</td>\n",
       "      <td>0.30010</td>\n",
       "      <td>0.14710</td>\n",
       "      <td>0.2419</td>\n",
       "      <td>0.07871</td>\n",
       "      <td>...</td>\n",
       "      <td>17.33</td>\n",
       "      <td>184.60</td>\n",
       "      <td>2019.0</td>\n",
       "      <td>0.16220</td>\n",
       "      <td>0.66560</td>\n",
       "      <td>0.7119</td>\n",
       "      <td>0.2654</td>\n",
       "      <td>0.4601</td>\n",
       "      <td>0.11890</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20.57</td>\n",
       "      <td>17.77</td>\n",
       "      <td>132.90</td>\n",
       "      <td>1326.0</td>\n",
       "      <td>0.08474</td>\n",
       "      <td>0.07864</td>\n",
       "      <td>0.08690</td>\n",
       "      <td>0.07017</td>\n",
       "      <td>0.1812</td>\n",
       "      <td>0.05667</td>\n",
       "      <td>...</td>\n",
       "      <td>23.41</td>\n",
       "      <td>158.80</td>\n",
       "      <td>1956.0</td>\n",
       "      <td>0.12380</td>\n",
       "      <td>0.18660</td>\n",
       "      <td>0.2416</td>\n",
       "      <td>0.1860</td>\n",
       "      <td>0.2750</td>\n",
       "      <td>0.08902</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>19.69</td>\n",
       "      <td>21.25</td>\n",
       "      <td>130.00</td>\n",
       "      <td>1203.0</td>\n",
       "      <td>0.10960</td>\n",
       "      <td>0.15990</td>\n",
       "      <td>0.19740</td>\n",
       "      <td>0.12790</td>\n",
       "      <td>0.2069</td>\n",
       "      <td>0.05999</td>\n",
       "      <td>...</td>\n",
       "      <td>25.53</td>\n",
       "      <td>152.50</td>\n",
       "      <td>1709.0</td>\n",
       "      <td>0.14440</td>\n",
       "      <td>0.42450</td>\n",
       "      <td>0.4504</td>\n",
       "      <td>0.2430</td>\n",
       "      <td>0.3613</td>\n",
       "      <td>0.08758</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>11.42</td>\n",
       "      <td>20.38</td>\n",
       "      <td>77.58</td>\n",
       "      <td>386.1</td>\n",
       "      <td>0.14250</td>\n",
       "      <td>0.28390</td>\n",
       "      <td>0.24140</td>\n",
       "      <td>0.10520</td>\n",
       "      <td>0.2597</td>\n",
       "      <td>0.09744</td>\n",
       "      <td>...</td>\n",
       "      <td>26.50</td>\n",
       "      <td>98.87</td>\n",
       "      <td>567.7</td>\n",
       "      <td>0.20980</td>\n",
       "      <td>0.86630</td>\n",
       "      <td>0.6869</td>\n",
       "      <td>0.2575</td>\n",
       "      <td>0.6638</td>\n",
       "      <td>0.17300</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>20.29</td>\n",
       "      <td>14.34</td>\n",
       "      <td>135.10</td>\n",
       "      <td>1297.0</td>\n",
       "      <td>0.10030</td>\n",
       "      <td>0.13280</td>\n",
       "      <td>0.19800</td>\n",
       "      <td>0.10430</td>\n",
       "      <td>0.1809</td>\n",
       "      <td>0.05883</td>\n",
       "      <td>...</td>\n",
       "      <td>16.67</td>\n",
       "      <td>152.20</td>\n",
       "      <td>1575.0</td>\n",
       "      <td>0.13740</td>\n",
       "      <td>0.20500</td>\n",
       "      <td>0.4000</td>\n",
       "      <td>0.1625</td>\n",
       "      <td>0.2364</td>\n",
       "      <td>0.07678</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>564</th>\n",
       "      <td>21.56</td>\n",
       "      <td>22.39</td>\n",
       "      <td>142.00</td>\n",
       "      <td>1479.0</td>\n",
       "      <td>0.11100</td>\n",
       "      <td>0.11590</td>\n",
       "      <td>0.24390</td>\n",
       "      <td>0.13890</td>\n",
       "      <td>0.1726</td>\n",
       "      <td>0.05623</td>\n",
       "      <td>...</td>\n",
       "      <td>26.40</td>\n",
       "      <td>166.10</td>\n",
       "      <td>2027.0</td>\n",
       "      <td>0.14100</td>\n",
       "      <td>0.21130</td>\n",
       "      <td>0.4107</td>\n",
       "      <td>0.2216</td>\n",
       "      <td>0.2060</td>\n",
       "      <td>0.07115</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>565</th>\n",
       "      <td>20.13</td>\n",
       "      <td>28.25</td>\n",
       "      <td>131.20</td>\n",
       "      <td>1261.0</td>\n",
       "      <td>0.09780</td>\n",
       "      <td>0.10340</td>\n",
       "      <td>0.14400</td>\n",
       "      <td>0.09791</td>\n",
       "      <td>0.1752</td>\n",
       "      <td>0.05533</td>\n",
       "      <td>...</td>\n",
       "      <td>38.25</td>\n",
       "      <td>155.00</td>\n",
       "      <td>1731.0</td>\n",
       "      <td>0.11660</td>\n",
       "      <td>0.19220</td>\n",
       "      <td>0.3215</td>\n",
       "      <td>0.1628</td>\n",
       "      <td>0.2572</td>\n",
       "      <td>0.06637</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>566</th>\n",
       "      <td>16.60</td>\n",
       "      <td>28.08</td>\n",
       "      <td>108.30</td>\n",
       "      <td>858.1</td>\n",
       "      <td>0.08455</td>\n",
       "      <td>0.10230</td>\n",
       "      <td>0.09251</td>\n",
       "      <td>0.05302</td>\n",
       "      <td>0.1590</td>\n",
       "      <td>0.05648</td>\n",
       "      <td>...</td>\n",
       "      <td>34.12</td>\n",
       "      <td>126.70</td>\n",
       "      <td>1124.0</td>\n",
       "      <td>0.11390</td>\n",
       "      <td>0.30940</td>\n",
       "      <td>0.3403</td>\n",
       "      <td>0.1418</td>\n",
       "      <td>0.2218</td>\n",
       "      <td>0.07820</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>567</th>\n",
       "      <td>20.60</td>\n",
       "      <td>29.33</td>\n",
       "      <td>140.10</td>\n",
       "      <td>1265.0</td>\n",
       "      <td>0.11780</td>\n",
       "      <td>0.27700</td>\n",
       "      <td>0.35140</td>\n",
       "      <td>0.15200</td>\n",
       "      <td>0.2397</td>\n",
       "      <td>0.07016</td>\n",
       "      <td>...</td>\n",
       "      <td>39.42</td>\n",
       "      <td>184.60</td>\n",
       "      <td>1821.0</td>\n",
       "      <td>0.16500</td>\n",
       "      <td>0.86810</td>\n",
       "      <td>0.9387</td>\n",
       "      <td>0.2650</td>\n",
       "      <td>0.4087</td>\n",
       "      <td>0.12400</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>568</th>\n",
       "      <td>7.76</td>\n",
       "      <td>24.54</td>\n",
       "      <td>47.92</td>\n",
       "      <td>181.0</td>\n",
       "      <td>0.05263</td>\n",
       "      <td>0.04362</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.1587</td>\n",
       "      <td>0.05884</td>\n",
       "      <td>...</td>\n",
       "      <td>30.37</td>\n",
       "      <td>59.16</td>\n",
       "      <td>268.6</td>\n",
       "      <td>0.08996</td>\n",
       "      <td>0.06444</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.2871</td>\n",
       "      <td>0.07039</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>569 rows × 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     mean radius  mean texture  mean perimeter  mean area  mean smoothness   \n",
       "0          17.99         10.38          122.80     1001.0          0.11840  \\\n",
       "1          20.57         17.77          132.90     1326.0          0.08474   \n",
       "2          19.69         21.25          130.00     1203.0          0.10960   \n",
       "3          11.42         20.38           77.58      386.1          0.14250   \n",
       "4          20.29         14.34          135.10     1297.0          0.10030   \n",
       "..           ...           ...             ...        ...              ...   \n",
       "564        21.56         22.39          142.00     1479.0          0.11100   \n",
       "565        20.13         28.25          131.20     1261.0          0.09780   \n",
       "566        16.60         28.08          108.30      858.1          0.08455   \n",
       "567        20.60         29.33          140.10     1265.0          0.11780   \n",
       "568         7.76         24.54           47.92      181.0          0.05263   \n",
       "\n",
       "     mean compactness  mean concavity  mean concave points  mean symmetry   \n",
       "0             0.27760         0.30010              0.14710         0.2419  \\\n",
       "1             0.07864         0.08690              0.07017         0.1812   \n",
       "2             0.15990         0.19740              0.12790         0.2069   \n",
       "3             0.28390         0.24140              0.10520         0.2597   \n",
       "4             0.13280         0.19800              0.10430         0.1809   \n",
       "..                ...             ...                  ...            ...   \n",
       "564           0.11590         0.24390              0.13890         0.1726   \n",
       "565           0.10340         0.14400              0.09791         0.1752   \n",
       "566           0.10230         0.09251              0.05302         0.1590   \n",
       "567           0.27700         0.35140              0.15200         0.2397   \n",
       "568           0.04362         0.00000              0.00000         0.1587   \n",
       "\n",
       "     mean fractal dimension  ...  worst texture  worst perimeter  worst area   \n",
       "0                   0.07871  ...          17.33           184.60      2019.0  \\\n",
       "1                   0.05667  ...          23.41           158.80      1956.0   \n",
       "2                   0.05999  ...          25.53           152.50      1709.0   \n",
       "3                   0.09744  ...          26.50            98.87       567.7   \n",
       "4                   0.05883  ...          16.67           152.20      1575.0   \n",
       "..                      ...  ...            ...              ...         ...   \n",
       "564                 0.05623  ...          26.40           166.10      2027.0   \n",
       "565                 0.05533  ...          38.25           155.00      1731.0   \n",
       "566                 0.05648  ...          34.12           126.70      1124.0   \n",
       "567                 0.07016  ...          39.42           184.60      1821.0   \n",
       "568                 0.05884  ...          30.37            59.16       268.6   \n",
       "\n",
       "     worst smoothness  worst compactness  worst concavity   \n",
       "0             0.16220            0.66560           0.7119  \\\n",
       "1             0.12380            0.18660           0.2416   \n",
       "2             0.14440            0.42450           0.4504   \n",
       "3             0.20980            0.86630           0.6869   \n",
       "4             0.13740            0.20500           0.4000   \n",
       "..                ...                ...              ...   \n",
       "564           0.14100            0.21130           0.4107   \n",
       "565           0.11660            0.19220           0.3215   \n",
       "566           0.11390            0.30940           0.3403   \n",
       "567           0.16500            0.86810           0.9387   \n",
       "568           0.08996            0.06444           0.0000   \n",
       "\n",
       "     worst concave points  worst symmetry  worst fractal dimension  target  \n",
       "0                  0.2654          0.4601                  0.11890       0  \n",
       "1                  0.1860          0.2750                  0.08902       0  \n",
       "2                  0.2430          0.3613                  0.08758       0  \n",
       "3                  0.2575          0.6638                  0.17300       0  \n",
       "4                  0.1625          0.2364                  0.07678       0  \n",
       "..                    ...             ...                      ...     ...  \n",
       "564                0.2216          0.2060                  0.07115       0  \n",
       "565                0.1628          0.2572                  0.06637       0  \n",
       "566                0.1418          0.2218                  0.07820       0  \n",
       "567                0.2650          0.4087                  0.12400       0  \n",
       "568                0.0000          0.2871                  0.07039       1  \n",
       "\n",
       "[569 rows x 31 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cancer_df = pd.DataFrame(data=features, columns=dataset.feature_names)\n",
    "cancer_df['target'] = labels\n",
    "cancer_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['malignant' 'benign']\n",
      "target\n",
      "1    357\n",
      "0    212\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# check dataset\n",
    "print(dataset.target_names)\n",
    "print(cancer_df['target'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cancer_df에서 feature용 Dataframe과 Label용 Series 객체 추출\n",
    "# 맨 마지막 칼럼이 Label, Feature용 Dataframe은 cancer_df의 첫번째 칼럼에서 맨 마지막 두번째 칼럼까지를 :-1 슬라이싱으로 추출\n",
    "\n",
    "x_features = cancer_df.iloc[:, :-1]\n",
    "y_label = cancer_df.iloc[:, -1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(455, 30) (114, 30)\n",
      "(409, 30) (46, 30)\n"
     ]
    }
   ],
   "source": [
    "# 전체 데이터 중 80%는 학습용 데이터, 20%는 테스트용 데이터로 분류\n",
    "x_train, x_test, y_train, y_test = train_test_split(x_features, y_label, test_size=0.2, random_state=156)\n",
    "\n",
    "# 위에서 만든 x_train, y_train을 다시 쪼개서 90%는 학습, 10%는 검증용 데이터로 분류\n",
    "x_tr, x_val, y_tr, y_val = train_test_split(x_train, y_train, test_size=0.1, random_state=156)\n",
    "\n",
    "print(x_train.shape, x_test.shape)\n",
    "print(x_tr.shape, x_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m코드를 실행할 수 없습니다. 세션이 삭제되었습니다. 커널을 다시 시작해 보세요."
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m현재 셀 또는 이전 셀에서 코드를 실행하는 동안 Kernel이 충돌했습니다. 셀의 코드를 검토하여 오류의 가능한 원인을 식별하세요. 자세한 내용을 보려면 <a href='https://aka.ms/vscodeJupyterKernelCrash'> 여기 </a> 를 클릭하세요. 자세한 내용은 Jupyter <a href='command:jupyter.viewOutput'>로그</a>를 참조하세요."
     ]
    }
   ],
   "source": [
    "# 학습, 검증, 테스트용 DMatrix 생성\n",
    "\n",
    "dtr = xgb.DMatrix(data=x_tr, label=y_tr)\n",
    "dval = xgb.DMatrix(data=x_val, label=y_val)\n",
    "dtest = xgb.DMatrix(data=x_test, label=y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# max_depth  = 3\n",
    "# 학습률 eta는 0,1\n",
    "# 예제 데이터가 0 또는 1의 이진 분류이므로 목적함수는 binary logistic\n",
    "# 오류 함수의 펴 ㅇ가 성능 지표는 logloss\n",
    "# num_rounds는 400회\n",
    "\n",
    "params = {'max_depth':3,\n",
    "            'eta':0.05,\n",
    "            'objective':'binary:logistic',\n",
    "            'eval_metric':'logloss'}\n",
    "\n",
    "num_rounds = 400"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\ttrain-logloss:0.65016\teval-logloss:0.66183\n",
      "[1]\ttrain-logloss:0.61131\teval-logloss:0.63609\n",
      "[2]\ttrain-logloss:0.57563\teval-logloss:0.61144\n",
      "[3]\ttrain-logloss:0.54310\teval-logloss:0.59204\n",
      "[4]\ttrain-logloss:0.51323\teval-logloss:0.57329\n",
      "[5]\ttrain-logloss:0.48447\teval-logloss:0.55037\n",
      "[6]\ttrain-logloss:0.45796\teval-logloss:0.52930\n",
      "[7]\ttrain-logloss:0.43436\teval-logloss:0.51534\n",
      "[8]\ttrain-logloss:0.41150\teval-logloss:0.49718\n",
      "[9]\ttrain-logloss:0.39027\teval-logloss:0.48154\n",
      "[10]\ttrain-logloss:0.37128\teval-logloss:0.46990\n",
      "[11]\ttrain-logloss:0.35254\teval-logloss:0.45474\n",
      "[12]\ttrain-logloss:0.33528\teval-logloss:0.44229\n",
      "[13]\ttrain-logloss:0.31892\teval-logloss:0.42961\n",
      "[14]\ttrain-logloss:0.30439\teval-logloss:0.42065\n",
      "[15]\ttrain-logloss:0.29000\teval-logloss:0.40958\n",
      "[16]\ttrain-logloss:0.27651\teval-logloss:0.39887\n",
      "[17]\ttrain-logloss:0.26389\teval-logloss:0.39050\n",
      "[18]\ttrain-logloss:0.25210\teval-logloss:0.38254\n",
      "[19]\ttrain-logloss:0.24123\teval-logloss:0.37393\n",
      "[20]\ttrain-logloss:0.23076\teval-logloss:0.36789\n",
      "[21]\ttrain-logloss:0.22091\teval-logloss:0.36017\n",
      "[22]\ttrain-logloss:0.21155\teval-logloss:0.35421\n",
      "[23]\ttrain-logloss:0.20263\teval-logloss:0.34683\n",
      "[24]\ttrain-logloss:0.19434\teval-logloss:0.34111\n",
      "[25]\ttrain-logloss:0.18637\teval-logloss:0.33634\n",
      "[26]\ttrain-logloss:0.17875\teval-logloss:0.33082\n",
      "[27]\ttrain-logloss:0.17167\teval-logloss:0.32675\n",
      "[28]\ttrain-logloss:0.16481\teval-logloss:0.32099\n",
      "[29]\ttrain-logloss:0.15835\teval-logloss:0.31671\n",
      "[30]\ttrain-logloss:0.15225\teval-logloss:0.31277\n",
      "[31]\ttrain-logloss:0.14650\teval-logloss:0.30882\n",
      "[32]\ttrain-logloss:0.14102\teval-logloss:0.30437\n",
      "[33]\ttrain-logloss:0.13590\teval-logloss:0.30103\n",
      "[34]\ttrain-logloss:0.13109\teval-logloss:0.29794\n",
      "[35]\ttrain-logloss:0.12647\teval-logloss:0.29499\n",
      "[36]\ttrain-logloss:0.12197\teval-logloss:0.29295\n",
      "[37]\ttrain-logloss:0.11784\teval-logloss:0.29043\n",
      "[38]\ttrain-logloss:0.11379\teval-logloss:0.28927\n",
      "[39]\ttrain-logloss:0.10994\teval-logloss:0.28578\n",
      "[40]\ttrain-logloss:0.10638\teval-logloss:0.28364\n",
      "[41]\ttrain-logloss:0.10302\teval-logloss:0.28183\n",
      "[42]\ttrain-logloss:0.09963\teval-logloss:0.28005\n",
      "[43]\ttrain-logloss:0.09649\teval-logloss:0.27972\n",
      "[44]\ttrain-logloss:0.09359\teval-logloss:0.27744\n",
      "[45]\ttrain-logloss:0.09080\teval-logloss:0.27542\n",
      "[46]\ttrain-logloss:0.08807\teval-logloss:0.27504\n",
      "[47]\ttrain-logloss:0.08541\teval-logloss:0.27458\n",
      "[48]\ttrain-logloss:0.08299\teval-logloss:0.27348\n",
      "[49]\ttrain-logloss:0.08035\teval-logloss:0.27247\n",
      "[50]\ttrain-logloss:0.07786\teval-logloss:0.27163\n",
      "[51]\ttrain-logloss:0.07550\teval-logloss:0.27094\n",
      "[52]\ttrain-logloss:0.07344\teval-logloss:0.26967\n",
      "[53]\ttrain-logloss:0.07147\teval-logloss:0.27008\n",
      "[54]\ttrain-logloss:0.06964\teval-logloss:0.26890\n",
      "[55]\ttrain-logloss:0.06766\teval-logloss:0.26854\n",
      "[56]\ttrain-logloss:0.06591\teval-logloss:0.26900\n",
      "[57]\ttrain-logloss:0.06433\teval-logloss:0.26790\n",
      "[58]\ttrain-logloss:0.06259\teval-logloss:0.26663\n",
      "[59]\ttrain-logloss:0.06107\teval-logloss:0.26743\n",
      "[60]\ttrain-logloss:0.05957\teval-logloss:0.26610\n",
      "[61]\ttrain-logloss:0.05817\teval-logloss:0.26644\n",
      "[62]\ttrain-logloss:0.05691\teval-logloss:0.26673\n",
      "[63]\ttrain-logloss:0.05550\teval-logloss:0.26550\n",
      "[64]\ttrain-logloss:0.05422\teval-logloss:0.26443\n",
      "[65]\ttrain-logloss:0.05311\teval-logloss:0.26500\n",
      "[66]\ttrain-logloss:0.05207\teval-logloss:0.26591\n",
      "[67]\ttrain-logloss:0.05093\teval-logloss:0.26501\n",
      "[68]\ttrain-logloss:0.04976\teval-logloss:0.26435\n",
      "[69]\ttrain-logloss:0.04872\teval-logloss:0.26360\n",
      "[70]\ttrain-logloss:0.04776\teval-logloss:0.26319\n",
      "[71]\ttrain-logloss:0.04680\teval-logloss:0.26255\n",
      "[72]\ttrain-logloss:0.04580\teval-logloss:0.26204\n",
      "[73]\ttrain-logloss:0.04484\teval-logloss:0.26254\n",
      "[74]\ttrain-logloss:0.04388\teval-logloss:0.26289\n",
      "[75]\ttrain-logloss:0.04309\teval-logloss:0.26249\n",
      "[76]\ttrain-logloss:0.04224\teval-logloss:0.26217\n",
      "[77]\ttrain-logloss:0.04133\teval-logloss:0.26166\n",
      "[78]\ttrain-logloss:0.04050\teval-logloss:0.26179\n",
      "[79]\ttrain-logloss:0.03967\teval-logloss:0.26103\n",
      "[80]\ttrain-logloss:0.03876\teval-logloss:0.26094\n",
      "[81]\ttrain-logloss:0.03806\teval-logloss:0.26148\n",
      "[82]\ttrain-logloss:0.03740\teval-logloss:0.26054\n",
      "[83]\ttrain-logloss:0.03676\teval-logloss:0.25967\n",
      "[84]\ttrain-logloss:0.03605\teval-logloss:0.25905\n",
      "[85]\ttrain-logloss:0.03545\teval-logloss:0.26007\n",
      "[86]\ttrain-logloss:0.03489\teval-logloss:0.25984\n",
      "[87]\ttrain-logloss:0.03425\teval-logloss:0.25933\n",
      "[88]\ttrain-logloss:0.03361\teval-logloss:0.25932\n",
      "[89]\ttrain-logloss:0.03311\teval-logloss:0.26002\n",
      "[90]\ttrain-logloss:0.03260\teval-logloss:0.25936\n",
      "[91]\ttrain-logloss:0.03202\teval-logloss:0.25886\n",
      "[92]\ttrain-logloss:0.03152\teval-logloss:0.25918\n",
      "[93]\ttrain-logloss:0.03107\teval-logloss:0.25864\n",
      "[94]\ttrain-logloss:0.03049\teval-logloss:0.25951\n",
      "[95]\ttrain-logloss:0.03007\teval-logloss:0.26091\n",
      "[96]\ttrain-logloss:0.02963\teval-logloss:0.26014\n",
      "[97]\ttrain-logloss:0.02913\teval-logloss:0.25974\n",
      "[98]\ttrain-logloss:0.02866\teval-logloss:0.25937\n",
      "[99]\ttrain-logloss:0.02829\teval-logloss:0.25893\n",
      "[100]\ttrain-logloss:0.02789\teval-logloss:0.25928\n",
      "[101]\ttrain-logloss:0.02751\teval-logloss:0.25955\n",
      "[102]\ttrain-logloss:0.02714\teval-logloss:0.25901\n",
      "[103]\ttrain-logloss:0.02668\teval-logloss:0.25991\n",
      "[104]\ttrain-logloss:0.02634\teval-logloss:0.25950\n",
      "[105]\ttrain-logloss:0.02594\teval-logloss:0.25924\n",
      "[106]\ttrain-logloss:0.02556\teval-logloss:0.25901\n",
      "[107]\ttrain-logloss:0.02522\teval-logloss:0.25738\n",
      "[108]\ttrain-logloss:0.02492\teval-logloss:0.25702\n",
      "[109]\ttrain-logloss:0.02453\teval-logloss:0.25789\n",
      "[110]\ttrain-logloss:0.02418\teval-logloss:0.25770\n",
      "[111]\ttrain-logloss:0.02384\teval-logloss:0.25842\n",
      "[112]\ttrain-logloss:0.02356\teval-logloss:0.25810\n",
      "[113]\ttrain-logloss:0.02322\teval-logloss:0.25848\n",
      "[114]\ttrain-logloss:0.02290\teval-logloss:0.25833\n",
      "[115]\ttrain-logloss:0.02260\teval-logloss:0.25820\n",
      "[116]\ttrain-logloss:0.02229\teval-logloss:0.25905\n",
      "[117]\ttrain-logloss:0.02204\teval-logloss:0.25878\n",
      "[118]\ttrain-logloss:0.02176\teval-logloss:0.25728\n",
      "[119]\ttrain-logloss:0.02149\teval-logloss:0.25722\n",
      "[120]\ttrain-logloss:0.02119\teval-logloss:0.25764\n",
      "[121]\ttrain-logloss:0.02095\teval-logloss:0.25761\n",
      "[122]\ttrain-logloss:0.02067\teval-logloss:0.25832\n",
      "[123]\ttrain-logloss:0.02045\teval-logloss:0.25808\n",
      "[124]\ttrain-logloss:0.02023\teval-logloss:0.25855\n",
      "[125]\ttrain-logloss:0.01998\teval-logloss:0.25714\n",
      "[126]\ttrain-logloss:0.01973\teval-logloss:0.25587\n",
      "[127]\ttrain-logloss:0.01946\teval-logloss:0.25640\n",
      "[128]\ttrain-logloss:0.01927\teval-logloss:0.25685\n",
      "[129]\ttrain-logloss:0.01908\teval-logloss:0.25665\n",
      "[130]\ttrain-logloss:0.01886\teval-logloss:0.25712\n",
      "[131]\ttrain-logloss:0.01863\teval-logloss:0.25609\n",
      "[132]\ttrain-logloss:0.01839\teval-logloss:0.25649\n",
      "[133]\ttrain-logloss:0.01816\teval-logloss:0.25789\n",
      "[134]\ttrain-logloss:0.01802\teval-logloss:0.25811\n",
      "[135]\ttrain-logloss:0.01785\teval-logloss:0.25794\n",
      "[136]\ttrain-logloss:0.01763\teval-logloss:0.25876\n",
      "[137]\ttrain-logloss:0.01748\teval-logloss:0.25884\n",
      "[138]\ttrain-logloss:0.01732\teval-logloss:0.25867\n",
      "[139]\ttrain-logloss:0.01719\teval-logloss:0.25876\n",
      "[140]\ttrain-logloss:0.01696\teval-logloss:0.25987\n",
      "[141]\ttrain-logloss:0.01681\teval-logloss:0.25960\n",
      "[142]\ttrain-logloss:0.01669\teval-logloss:0.25982\n",
      "[143]\ttrain-logloss:0.01656\teval-logloss:0.25992\n",
      "[144]\ttrain-logloss:0.01638\teval-logloss:0.26035\n",
      "[145]\ttrain-logloss:0.01623\teval-logloss:0.26055\n",
      "[146]\ttrain-logloss:0.01606\teval-logloss:0.26092\n",
      "[147]\ttrain-logloss:0.01589\teval-logloss:0.26137\n",
      "[148]\ttrain-logloss:0.01572\teval-logloss:0.25999\n",
      "[149]\ttrain-logloss:0.01556\teval-logloss:0.26028\n",
      "[150]\ttrain-logloss:0.01546\teval-logloss:0.26048\n",
      "[151]\ttrain-logloss:0.01531\teval-logloss:0.26142\n",
      "[152]\ttrain-logloss:0.01515\teval-logloss:0.26188\n",
      "[153]\ttrain-logloss:0.01501\teval-logloss:0.26227\n",
      "[154]\ttrain-logloss:0.01486\teval-logloss:0.26287\n",
      "[155]\ttrain-logloss:0.01476\teval-logloss:0.26299\n",
      "[156]\ttrain-logloss:0.01462\teval-logloss:0.26346\n",
      "[157]\ttrain-logloss:0.01448\teval-logloss:0.26379\n",
      "[158]\ttrain-logloss:0.01434\teval-logloss:0.26306\n",
      "[159]\ttrain-logloss:0.01424\teval-logloss:0.26237\n",
      "[160]\ttrain-logloss:0.01410\teval-logloss:0.26251\n",
      "[161]\ttrain-logloss:0.01401\teval-logloss:0.26265\n",
      "[162]\ttrain-logloss:0.01392\teval-logloss:0.26264\n",
      "[163]\ttrain-logloss:0.01380\teval-logloss:0.26250\n",
      "[164]\ttrain-logloss:0.01372\teval-logloss:0.26264\n",
      "[165]\ttrain-logloss:0.01359\teval-logloss:0.26255\n",
      "[166]\ttrain-logloss:0.01350\teval-logloss:0.26188\n",
      "[167]\ttrain-logloss:0.01342\teval-logloss:0.26203\n",
      "[168]\ttrain-logloss:0.01331\teval-logloss:0.26190\n",
      "[169]\ttrain-logloss:0.01319\teval-logloss:0.26184\n",
      "[170]\ttrain-logloss:0.01312\teval-logloss:0.26133\n",
      "[171]\ttrain-logloss:0.01304\teval-logloss:0.26148\n",
      "[172]\ttrain-logloss:0.01297\teval-logloss:0.26157\n",
      "[173]\ttrain-logloss:0.01285\teval-logloss:0.26253\n",
      "[174]\ttrain-logloss:0.01278\teval-logloss:0.26229\n",
      "[175]\ttrain-logloss:0.01267\teval-logloss:0.26086\n"
     ]
    }
   ],
   "source": [
    "# 학습 데이터 셋은 'train' 또는 평가 데이터 셋은 'eval'로 표기\n",
    "eval_list = [(dtr, 'train'),(dval,'eval')]\n",
    "\n",
    "# 하이퍼 파라미터와 early stopping 파라미터를 train() 함수의 파라미터로 전달\n",
    "xgb_model = xgb.train(params=params, dtrain=dtr, num_boost_round=num_rounds, early_stopping_rounds=50, evals=eval_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predict( ) 수행 결과를 10개만 표시, 예측 확률 값으로 표시됨\n",
      "[1. 0. 1. 0. 1. 1. 1. 1. 1. 0.]\n",
      "예측값 10개만 표시:  [1, 0, 1, 0, 1, 1, 1, 1, 1, 0]\n"
     ]
    }
   ],
   "source": [
    "pred_probs = xgb_model.predict(dtest)\n",
    "print('predict( ) 수행 결과를 10개만 표시, 예측 확률 값으로 표시됨')\n",
    "print(np.round(pred_probs[:10]))\n",
    "\n",
    "# 에측 확률이 0.5보다 크면 1, 그렇지 않으면 0으로 예측값을 결정하여 List 객체인 preds에 저장\n",
    "preds = [1 if x>0.5 else 0 for x in pred_probs]\n",
    "print('예측값 10개만 표시: ', preds[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import clf_eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "get_clf_eval() takes 2 positional arguments but 3 were given",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[32], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m clf_eval\u001b[39m.\u001b[39;49mget_clf_eval(y_test, preds, pred_probs)\n",
      "\u001b[0;31mTypeError\u001b[0m: get_clf_eval() takes 2 positional arguments but 3 were given"
     ]
    }
   ],
   "source": [
    "clf_eval.get_clf_eval(y_test, preds, pred_probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "test",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
